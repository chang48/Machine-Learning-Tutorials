{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network basics<a id=\"Top\"></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "Table of Content\n",
    "<ul>\n",
    "<li>1. <a href=\"#Part_1\">The Perceptron</a></li>\n",
    "<li>2. <a href=\"#Part_2\">Perceptron in action</a></li>\n",
    "<li>3. <a href=\"#Part_3\">The XOR classification problem</a></li>\n",
    "<li>4. <a href=\"#Part_4\">Multi-Layer Perceptron</a></li>    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from IPython.display import HTML, display\n",
    "\n",
    "# Define overall figure size. Use the Golden ratio to compute width and height.\n",
    "height = 6.0\n",
    "width = height*0.5*(1.0+np.sqrt(5.0))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (width, height)\n",
    "plt.rcParams['mathtext.fontset'] = 'cm'\n",
    "plt.rcParams['mathtext.rm'] = 'serif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Perceptron\n",
    "\n",
    "__Linear Threshold Unit (LTU)__ is a design of artificial neuron. Its structure is depicted as follows\n",
    "\n",
    "<img src=\"./images/fig_LTU.png\" width='300'>\n",
    "\n",
    "A LTU takes inputs (in this example: $x_0$, $x_1$, and $x_2$), applies an activation function on a weighted sum of the inputs, then outputs the results. Written in mathematics, the procedure can be expressed as\n",
    "\n",
    "$$ \\mbox{Output}=h_{\\mathbf{w}}(z),\\,\\,\\,\\,\\,\\mbox{where}\\,\\,\\,z = w_1 x_1 + w_2 x_2 + w_3 x_3 $$\n",
    "\n",
    "Basically the LTU is performing a transformation on the inputs. In fact, this is exactly what neural networks do under the hood. The difference being large complex networks are carrying out many layers of non-linear transformations. For $n$ inputs, the weighted sum is\n",
    "\n",
    "$$ z = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n = \\mathbf{w}^T\\cdot\\mathbf{x}, $$\n",
    "\n",
    "where in the last term we have used the vector notation. Typically $x_0=1$ is the bias term. Each input $x_i$ is associated with a weight $w_i$. There are two common activation functions\n",
    "\n",
    "- Heaviside step function \n",
    "\n",
    "$$\n",
    "h(z) = \\left\\{\\begin{array}{cc}\n",
    "               0 & \\mbox{if}\\,\\, z < 0,\\\\\n",
    "               1 & \\mbox{if}\\,\\, z \\geq 0.\n",
    "               \\end{array}\n",
    "        \\right. \n",
    "$$\n",
    "\n",
    "\n",
    "- Sign function\n",
    "\n",
    "$$ \\mbox{sgn}(z) = \\left\\{\\begin{array}{cc}\n",
    "                 -1 & \\mbox{if}\\,\\, z < 0,\\\\\n",
    "                  0 & \\mbox{if}\\,\\, z = 0, \\\\\n",
    "                  1 & \\mbox{if}\\,\\, z > 0.\n",
    "               \\end{array}\n",
    "        \\right. \n",
    "$$\n",
    "    \n",
    "A __Perceptron__ is composed of a single layer of LTUs with each neuron connected to all the inputs. The following figure shows a Perceptron composed of three input neurons and three output neurons.\n",
    "\n",
    "<img src=\"./images/fig_perceptron.png\" width='300'>\n",
    "\n",
    "How is a perceptron trained? The basic idea is \"__Cells that fire together wire together__,\" which is also refereed to as __Hebb's rule__. The rule states that the connection weight between two neurons is increased whenever they have the same output. The enfore the rule, the weight is updated as follows\n",
    "\n",
    "$$ w_{i,j}' = w_{i,j} + \\eta\\,(\\hat y_j - y_j)\\,x_i, $$\n",
    "\n",
    "where\n",
    "- $w_{i,j}$ is the connection weight between the i-th input neuron and the j-th output neuron.\n",
    "- $x_i$ is the input value of the current training instance.\n",
    "- $\\hat y_i$ is the output of the j-th output neuron for the current training instance.\n",
    "- $j_i$ is the target ouput of the j-th output neuron for the current training instance.\n",
    "- $\\eta$ is the learning rate.\n",
    "\n",
    "So for every output neuron that gives a wrong prediction, the equation reinforces the connection weights from the inputs that would have contributed to the correct prediction.\n",
    "\n",
    "Several notable properties of a single perceptron:\n",
    "- It is a type of feed-forward network.\n",
    "- It is a __binary__ classifier. This is because the output is either positive (+1) or negative (0, in the case of step function or -1, in the case of sign function).\n",
    "- It has a linear decision boundary defined by the condition $\\mathbf{w}^T\\cdot\\mathbf{x}=0$. In other words, the perceptron has only one decision boundary. \n",
    "- Scikit-Learn's perceptron implementation resembles its SGD class. In fact, the SGD classifier with `loss=\"perceptron\"`, `learning_rate=\"constant\"`, `eta0=1`, and `penalty=None` hyperparameters is equivalent to Scikit-Learn's peceptron class.\n",
    "- Peceptrons do not output class probability. They just make predictions based on threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron in action<a id=\"Part_2\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>\n",
    "\n",
    "Let's get our hands dirty by actually train a Perceptron. The following code snippet is a simple Python implementation of a Perceptron. It is used to classified a linearly separable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial weight: [ -3.72   -1.02   -0.97]\n",
      "\n",
      "Epoch:   1,  error:  0.500,   weight: [ -3.42   -0.97   -0.87]\n",
      "Epoch:   2,  error:  0.500,   weight: [ -3.12   -0.92   -0.77]\n",
      "Epoch:   3,  error:  0.500,   weight: [ -2.82   -0.87   -0.67]\n",
      "Epoch:   4,  error:  0.500,   weight: [ -2.52   -0.82   -0.57]\n",
      "Epoch:   5,  error:  0.500,   weight: [ -2.22   -0.77   -0.47]\n",
      "Epoch:   6,  error:  0.500,   weight: [ -1.92   -0.72   -0.37]\n",
      "Epoch:   7,  error:  0.500,   weight: [ -1.62   -0.67   -0.27]\n",
      "Epoch:   8,  error:  0.667,   weight: [ -1.42   -0.42   -0.07]\n",
      "Epoch:   9,  error:  0.500,   weight: [ -1.12   -0.37    0.03]\n",
      "Epoch:  10,  error:  0.500,   weight: [ -0.82   -0.32    0.13]\n",
      "Epoch:  11,  error:  0.500,   weight: [ -0.52   -0.27    0.23]\n",
      "Epoch:  12,  error:  0.500,   weight: [ -0.22   -0.22    0.33]\n",
      "Epoch:  13,  error:  0.333,   weight: [ -0.02    0.03    0.33]\n",
      "Epoch:  14,  error:  0.333,   weight: [  0.18   -0.02    0.33]\n",
      "Epoch:  15,  error:  0.167,   weight: [  0.28    0.13    0.23]\n",
      "Epoch:  16,  error:  0.333,   weight: [  0.28    0.08    0.33]\n",
      "Epoch:  17,  error:  0.000,   weight: [  0.28    0.08    0.33]\n",
      "\n",
      "Decision boundary interceptions: \n",
      "x1:  -3.663, x2:  -0.849\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 698.991x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linearly separable Data sets\n",
    "#X = [[1,1], [2,2], [4,4], [5,5], [5,4]]\n",
    "#y = [1, 1, 0, 0, 0]\n",
    "\n",
    "X = [[1,1], [2,-2], [1.5,-1], [-2,1], [-1,-1.5], [-2,-1]]\n",
    "y = [1, 0, 1, 1, 0, 0]\n",
    "\n",
    "# XOR problrm data set\n",
    "#X = [[0,0], [1,0], [0,1], [1,1]]\n",
    "#y = [1, 0, 0, 1]\n",
    "\n",
    "# Add bias term to X\n",
    "Xb = np.c_[np.ones((np.shape(X)[0], 1)), X]    \n",
    "\n",
    "# Initialize random weight\n",
    "weight = 5*np.random.random_sample((3)) - 5\n",
    "eta    = 0.1\n",
    "epoch  = 2000\n",
    "\n",
    "idx = 0\n",
    "print('\\nInitial weight: ['+'  '.join('{:6.2f}'.format(f) for f in weight)+']\\n')\n",
    "\n",
    "# Single perceptron training\n",
    "for iter in range(epoch):\n",
    "    error = 0.\n",
    "    for x, yt in zip(Xb, y):\n",
    "        y_pred = np.heaviside(x.dot(weight), 0.0)\n",
    "        err = yt - y_pred\n",
    "        weight = weight + eta*err*x\n",
    "        error += np.abs(err)/6.\n",
    "    idx += 1\n",
    "    print('Epoch: {0:3d},  error: {1:6.3f},'.format(idx, error) + \\\n",
    "          '   weight: ['+'  '.join('{:6.2f}'.format(f) for f in weight)+']')\n",
    "    \n",
    "    # The data set is linearly separable. So we use zero error as the exit condition.\n",
    "    if (error < 1.e-6):\n",
    "        break\n",
    "\n",
    "print('\\nDecision boundary interceptions: ')\n",
    "print('x1:{0:8.3f}, x2:{1:8.3f}'.format(-weight[0]/weight[1], -weight[0]/weight[2]))\n",
    "\n",
    "# Plot data and decision boundary\n",
    "size = 400\n",
    "xmin = np.min(np.amin(X, axis=0))-1\n",
    "xmax = np.max(np.amax(X, axis=0))+1\n",
    "ymin = np.min(np.amin(X, axis=1))-1\n",
    "ymax = np.max(np.amax(X, axis=1))+1\n",
    "\n",
    "xx, yy = np.meshgrid( np.linspace(xmin, xmax, size), np.linspace(ymin, ymax, size))\n",
    "\n",
    "mesh   = np.c_[xx.ravel(), yy.ravel()]\n",
    "# Add bias term to the mesh\n",
    "meshb  = np.c_[np.ones((mesh.shape[0], 1)), mesh]\n",
    "z  = np.heaviside(meshb.dot(weight), 1.0)\n",
    "zz = np.reshape(z, xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(xx, yy, zz, cmap=plt.cm.brg, alpha=0.1)\n",
    "\n",
    "# Plot data points\n",
    "style = [ 'bo' if x > 0 else 'ro'  for x in y ]\n",
    "for x, c in zip(X, style):\n",
    "    plt.plot(x[0], x[1], c)   \n",
    "plt.hlines(0, xmin, xmax, color='k', lw=0.5)\n",
    "plt.vlines(0, ymax, ymin, color='k', lw=0.5)  \n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel(r'$x_1$', fontsize=22)\n",
    "plt.ylabel(r'$x_2$', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the simple example demonstrates how a Perceptron is trained. Most importantly, one can see from the figure that\n",
    "\n",
    "1. A Perceptron has only one decision boundary.\n",
    "2. The decision boundary of a Perceptron is linear. \n",
    "\n",
    "Apparently for a more complex data set, the Perceptron's linearity would suffer badly. To see why, let's move on the the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The XOR classification problem<a id=\"Part_3\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>\n",
    "\n",
    "The XOR classification problem is the one of using a neural network to predict the outputs of XOR logic gates given two binary inputs. The possible inputs and outs of XOR gates are\n",
    "\n",
    "| input 1 | input 2 | output |\n",
    "|---------|---------|--------|\n",
    "|     0   |     0   |    0   |   \n",
    "|     0   |     1   |    1   |\n",
    "|     1   |     1   |    0   |\n",
    "|     1   |     0   |    1   |\n",
    "\n",
    "This problem can also be demonstrated using the Cartesian coordinate system, shown as follows:\n",
    "\n",
    "<img src=\"./images/fig_XOR.png\" width='240'>\n",
    "\n",
    "The figure clearly shows that there is no way of seperating squares and triangles using one decision boundary. This is true for a single layer perceptron as well as for any linear classification models. __We can also demonstrate this issue by applying the single perceptron model we just built to the XOR data set.__ The training error will never converge to the minimum. \n",
    "\n",
    "An excellent and beautiful demonstration of the XOR problem can be found using the <a href=\"http://playground.tensorflow.org/#activation=linear&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=3,3,3&seed=0.52109&showTestData=false&discretize=false&percTrainData=80&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\">TensorFlow Playground</a>. As shown in the following figure, the link sets up a neural network with four layers. The input layer has two inputs and the output layer has three output neurons. When applied to the more involved XOR dataset, one can use the link to show that the network will never find a suitable decision boundary.\n",
    "\n",
    "<img src=\"./images/fig_DL01-XOR.png\" width='840'>\n",
    "\n",
    "This is because the activation function in our neural network example is linear. It can be easily shown that as long as the activation function is linear, the entire network is effectively doing a linear transformation. Since XOR is not a linearly separable problem, it is no surprise that our network would fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Layer Perceptron<a id=\"Part_4\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>\n",
    "\n",
    "A solution to the XOR problem is to stack multiple perceptron layers, forming the so-called multi-layer perceptron (MLP). A generic MLP is composed of one input layer, one or more layers of LTUs called hidden layers, and one final layer of LTUs called the output layer. The following figure is an example.\n",
    "\n",
    "<img src=\"./images/fig_MLP.png\" width='500'>\n",
    "\n",
    "The most important development in neural network training is the idea of backpropagation algorithm. It could be described as __gradient descent using reverse-mode autodiff.__ Here is a breakdown of the algorithm\n",
    "\n",
    "For each training instance $x$:\n",
    "- Feed $x$ to the network and computes the outputs of every neuron in each layer. That is, forward pass.\n",
    "- Measure the network's output error by comparing the desired output and actual output.\n",
    "- Computes how much each neuron in the last hidden layer contributed to each output neuron's error.\n",
    "- Estimates how much of these error contributions came from the previous layer.\n",
    "- Keep doing the same backward error estimation until the algorithm reaches the input layer.\n",
    "- The last step: apply Gradient Descent on all connection weights using the error gradients measured previously. So the weights get adjusted in order to redue the error.\n",
    "\n",
    "The reverse pass efficiently measures the error gradient across all the connection weights by __propagating the error gradient backward__ in the network. \n",
    "\n",
    "In order for the calculation of gradients to work, one needs to use a differentiable activation function. Common candidates are:\n",
    "- Logistic function $$f(z) = 1/(1+\\exp(-z)).$$\n",
    "- Hyperbolic tangent function $$f(z) = \\tanh(z).$$\n",
    "- ReLU function $$f(z) = \\mbox{max}(0,z).$$\n",
    "\n",
    "An MLP is often used for classification, with each output corresponding to a different binary class. __When the classes are exclusive, the softmax function is often shared across the output layer as the activation function.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
