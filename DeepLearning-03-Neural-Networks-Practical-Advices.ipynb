{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Practical Advices<a id=\"Top\"></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "Table of Content\n",
    "<ul>\n",
    "<li>1. <a href=\"#Part_1\">Vanishing/Exploding Gradients</a></li>\n",
    "<li>2. <a href=\"#Part_2\">Reusing Pretrained Layers</a></li>\n",
    "<li>3. <a href=\"#Part_3\">Faster Optimizers</a></li>\n",
    "<li>4. <a href=\"#Part_4\">Avoiding Overfitting Through Regularization</a></li>    \n",
    "</ul>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vanishing/Exploding Gradients\n",
    "\n",
    "From the previous chapter, we know that training a network amounts to determine the weights and biases of the net in such a way that the loss is minimal. The backpropagation technique allows us to work out the gradients from the output layer all the way to the input layer. Hence traning for each layer is possible because now we have a way of updating the network's trainable parameters, i.e. weights and biases. \n",
    "\n",
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the gradient descent update leaves the lower layers unchanged, and training never converges to a good solution. This is called the __vanishing gradient__ problem. In some cases, the opposite happens: the gradient gets bigger and bigger as we walk down the layers, creating the so called __exploding gradients__ problem.\n",
    "\n",
    "For the logistic activation, the problem can be understood as follows. When the input is large in value, the logistic function value saturates at either 1 or 0, with a derivative extremely close to zero. Thus when backpropagation kicks in, there is essentially no gradient to propagate back through the network.\n",
    "\n",
    "### Xavier and He Initialization\n",
    "\n",
    "The gradient vanishing/exploding problem has to do with how the network gets initialized and the type of activation function is implemented. The former issue can be tackled using the Xavier and He initialization method. The method argues that the connection weights must be initialized as follows\n",
    "\n",
    "| Activation Function | Uniform Distribution | Normal Distribution |\n",
    "|---------------------|----------------------|---------------------|\n",
    "| Logistic | $r = \\sqrt{\\displaystyle\\frac{6}{n_i + n_o}}$  | $\\sigma= \\sqrt{\\displaystyle\\frac{2}{n_i + n_o}}$  | \n",
    "| $\\tanh$  | $r = 4\\sqrt{\\displaystyle\\frac{6}{n_i + n_o}}$ | $\\sigma= 4\\sqrt{\\displaystyle\\frac{2}{n_i + n_o}}$ | \n",
    "| ReLU     | $r = \\sqrt{\\displaystyle\\frac{12}{n_i + n_o}}$ | $\\sigma= \\sqrt{\\displaystyle\\frac{4}{n_i + n_o}}$  | \n",
    "\n",
    "where $n_i$ and $n_o$ denote the number of input and output neurons. The strategy for the ReLU function is sometimes called __He Initialization__.\n",
    "\n",
    "### Nonsaturating Activation Functions\n",
    "\n",
    "As mentioned previously, the vanishing/exploding gradient problem also has to do with activation functions. For ReLU, in particular, a class of problem called __dying ReLU__ occurs. This is because during training, if the updated weights are negative, then the neurons will stop outputing anything except zero. When this happens, the neurons are unlikely to come back since the gradient of ReLU now is always zero when the outputs are negative. \n",
    "\n",
    "To solve the problem, one could adopt __leaky ReLU__ or __ELU__.\n",
    "\n",
    "$$\n",
    "  \\mbox{Leaky ReLU}(z) = \\max(\\alpha z, z)\n",
    "$$\n",
    "where $\\alpha$ is a hyperparameter. ELU is defined as follows\n",
    "\n",
    "$$ \n",
    "        \\mbox{ELU}(z) = \\left\\{\n",
    "           \\begin{array}{lc}\n",
    "               \\alpha(e^z-1 ) & \\mbox{if}\\,\\, z < 0,\\\\\n",
    "                           z  & \\mbox{if}\\,\\, z \\geq 0.\n",
    "            \\end{array}\n",
    "        \\right.\n",
    "$$\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "He initialization and ELU activation combined could reduce the gradient vanishing/exploding problem significantly. But there is no guarantee the problem will not resurface. Batch normalization was proposed in 2015 and it is shown to be able to considerably improve deep neural network learning convergence. \n",
    "\n",
    "The idea is to add an operation in the model before the activation function of each layer: zero-centering and normalizing the inputs, then scale and shift the results using two additional parameters per layer:\n",
    "\n",
    "$$ \n",
    "   \\begin{align}\n",
    "     \\mu_B       &= \\frac{1}{m_B}\\sum_{i=1}^{m_B}\\, \\mathbf{x}^{(i)} \\\\\n",
    "     \\sigma_B^2  &= \\frac{1}{m_B}\\sum_{i=1}^{m_B}\\, \\left( \\mathbf{x}^{(i)} - \\mu_B \\right)^2 \\\\\n",
    "     \\hat{\\mathbf{x}}^{(i)} &= \\frac{\\mathbf{x}^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2+\\epsilon}} \\\\\n",
    "     \\mathbf{z}^{(i)} &= \\gamma\\,\\mathbf{x}^{(i)} + \\beta\n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "Here\n",
    "- $\\mu_B$ is the empirical mean, computer over the whole mini-batch $B$.\n",
    "- $\\sigma_B$ is the standard deviation of the whole mini-batch $B$.\n",
    "- $m_B$ is the number of instances in the mini-batch.\n",
    "- $\\hat{\\mathbf{x}}^{(i)}$ is the zero-centered and normalized input.\n",
    "- $\\gamma$ is the scaling parameter for the layer under consideration.\n",
    "- $\\beta$ is the shifting parameter for the layer.\n",
    "- $\\epsilon$ is a tiny number (typically set to $10^{-3}$) to avoid division by zero.\n",
    "- $\\mathbf{z}^{(i)}$ is the output of the batch normalization operation.\n",
    "\n",
    "At test time, one just compute the mean and standard deviation of the whole training set. So in total, there are four additional hyperparameters for each batch-normalized layer: scale $\\gamma$, offset $\\beta$, mean $\\mu$, and standard deviation $\\sigma$.\n",
    "\n",
    "### Gradient Clipping\n",
    "\n",
    "Another popular technique to alleviate the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reusing Pretrained Layers<a id=\"Part_2\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>\n",
    "\n",
    "### Transfer learning in action\n",
    "\n",
    "In training neural networks, the goal is to determine, for each layer, the bias $b$ and weight $w$ for each neuron. These variables are often called collectively as *trainable variables*. There are cases where trainable variables contain more than just bias and weight. For example, when applying batch normalization, each layer has four additional trainable variables: scale $\\gamma$, offset $\\beta$, mean $\\mu$, and standard deviation $\\sigma$.\n",
    "\n",
    "So in this sense, transfer learning means reusing the trainable variables of a pretrained network. One way of doing this is just to reload the saved model and train it on the new task. However, it is often the case that we want to reuse part of the saved model, say trainable parameters from bottom layers. In tensorflow, how do we retrieve these variables from a pre-trained network? tensorflow provides `tf.get_collection()` function for this task:\n",
    "\n",
    "```python\n",
    "reuse_vars = tf.get_collection(key, scope=None)\n",
    "```\n",
    "\n",
    "Here `key` is the collection of variables we wish to retrieve. `scope` is a regular expression string that can be fed \n",
    "to `re.match`. Here is an application example from the Handson book:\n",
    "\n",
    "```python\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='hidden[123]'),\n",
    "```\n",
    "\n",
    "the key `tf.GraphKeys.TRAINABLE_VARIABLES` belongs to the `tf.GraphKeys` class that contains many standard variables. `hidden[123]` is a regular expression that produces any hidden layer whose namescope is `hidden1`, `hidden2`, or `hidden3`. Namely, we are getting hidden layers 1 to 3.\n",
    "\n",
    "Now with `tf.get_collection()` we retrieve the desired variables. The next step is to use `tf.Saver()` to load these variables from the pre-trained model. In particular, we need to feed `tf.Waver()` a dictionary that specifies the variables to be loaded. We now can piece toether the above code segments to write a program that reuses the first three hidden layers of a saved mode:\n",
    "```python\n",
    "[... build new model with the same definition as before the hidden layers 1-3 ...]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='hidden[123]')\n",
    "reuse_var_dict = dict([(var, var) for var in reuse_vars])\n",
    "original_saver = tf.Saver(reuse_var_dict)\n",
    "new_saver = tf.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    original_saver.restore('/path/to_the_saved_model.ckpt')\n",
    "    [... train the model ...]\n",
    "    new_saver.save('/path/to_the_new_model.ckpt')\n",
    "```\n",
    "The code does the following:\n",
    "1. Build the new model, making sure to copy the saved model's hidden layers 1-3.\n",
    "2. Initialize variables.\n",
    "3. Get all the trainable parameters from the saved model using `tf.get_collection()`.\n",
    "4. Create a dictionary that mapps the name of each variable in the saved model to its name in the new model.\n",
    "5. Create a saver that restores the saved model.\n",
    "6. Create a separate saver to save the new model.\n",
    "7. Train the new model.\n",
    "8. Save the new model.\n",
    "\n",
    "This process assumes that __both the saved and new models are performing the same or similar task__.\n",
    "\n",
    "### Freezing and caching lower layers\n",
    "\n",
    "There are times when the reused layers have already learned to detect low-level features of the dataset. In this case, it is possible to exclude them from training. These are called _frozen layers_. In tensorflow, we can use the following piece of code to achieve the goal:\n",
    "```python\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='hidden[34]|outputs')\n",
    "training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "```\n",
    "In this example, we just pass a list of variables to be trained (retrieved by calling `tf.get_collection()`) to the optimizer. That's it!\n",
    "​\n",
    "Now that layers 1 and 2 are frozen, we may run layer 2 through the entire dataset and store its outputs for the training of layers 3, 4 and output layers. This is possible because layer 2's outputs are fixed (remember that layer 1 and 2 will not be trained). Doing so will save a lot of training time, provided we have enough memory to store the outputs.\n",
    "\n",
    "### Practical advices\n",
    "- Normally, the output layer of saved model should be replaced, because it very likely will not work with the new task nor does it have the right number of outputs.\n",
    "- The upper hidden layers of the saved model sould also be replaced. This is because upper layers are trained to recognize higher level features, which are task-dependent.\n",
    "\n",
    "To find the right number of layers to use:\n",
    "1. Freeze all copied layers first, then train the model and evaluate its performance.\n",
    "2. Gradually unfreeze these layers and monitor the model's performance change. __The more training data you have, the more layers you can unfreeze__.\n",
    "3. If there is no improvement, try dropping top hidden layers and freeze all remaining hidden layers from the saved model again.\n",
    "\n",
    "Iterate these steps until one finds the right number of laers to reuse.\n",
    "\n",
    "### Getting pretrained models: Model Zoo\n",
    "1. Tensorflow (<a href=\"https://www.tensorflow.org/about/uses\">Link</a>). Eg. VGG, Inception, ResNet for image clasification.\n",
    "2. Caffe Model Zoo: (<a href=\"http://caffe.berkeleyvision.org/model_zoo.html\">Link</a>). Eg. LeNet, AlexNet, VGGNet, ...etc for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Faster Optimizers<a id=\"Part_3\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>\n",
    "\n",
    "### Momentum optimization\n",
    "Recall the Gradient Descent update:\n",
    "\n",
    "$$ \\theta' = \\theta - \\eta \\nabla_\\theta J(\\theta) $$\n",
    "\n",
    "The step size is fixed and the update does not care about what the earlier gradients were. If the local gradient is small, then the update will goes very slowly. \n",
    "\n",
    "Momentum optimization does exactly the opposize: it cares greatly about the previous gradients. At each iteration, it adds the local gradient to the momentum vector $\\mathbf{m}$, and updates the weights by subtracting the momentum vector:\n",
    "\n",
    "$$ \n",
    "   \\begin{align}\n",
    "     \\mathbf{m}' &= \\beta\\,\\mathbf{m} + \\eta \\nabla_\\theta J(\\theta) \\\\\n",
    "     \\theta' &= \\theta - \\mathbf{m} \n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "If the gradient remains constant, one can show that the maximal size of the weight updates is equal to that gradient multiplied by $\\eta / (1-\\beta)$. Let $G$ denote the gradient and $\\mathbf{m}^{(0)}$ the initial guess of the momentum vector. The update formula gives the following scheme:\n",
    "\n",
    "$$  \n",
    "  \\begin{align}\n",
    "    \\mathbf{m}^{(1)} &= \\beta\\,\\mathbf{m}^{(0)} + \\eta G\\\\\n",
    "    \\mathbf{m}^{(2)} &= \\beta\\,\\mathbf{m}^{(1)} + \\eta G\n",
    "                      = \\beta^2\\,\\mathbf{m}^{(0)} + \\beta\\eta G + \\eta G\\\\\n",
    "    \\mathbf{m}^{(3)} &= \\beta\\,\\mathbf{m}^{(2)} + \\eta G\n",
    "                      = \\beta^3\\,\\mathbf{m}^{(0)} + \\beta^2\\beta G + \\beta\\eta G + \\eta G\\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{m}^{(k)} &= \\beta^k\\,\\mathbf{m}^{(0)} + \n",
    "                        \\left( \n",
    "                            \\beta^{k-1} + \\ldots + \\beta^2 + \\beta + 1\n",
    "                        \\right) \\eta G\n",
    "  \\end{align}\n",
    "$$\n",
    "\n",
    "Then it's easy to show that the series in $\\beta$ converges to $1/(1-\\beta)$ for $\\beta < 1$.\n",
    "\n",
    "### Nesterov Accelerated Gradient\n",
    "This is essentially the momentum optimization with the only difference being that now the gradient is measured in the direction of the momentum:\n",
    "\n",
    "$$ \n",
    "   \\begin{align}\n",
    "     \\mathbf{m}' &= \\beta\\,\\mathbf{m} + \\eta \\nabla_\\theta J(\\theta+\\beta\\mathbf{m}) \\\\\n",
    "     \\theta' &= \\theta - \\mathbf{m} \n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "It is shown that this small modification leads to a faster optimization than the plain vanilla momentum optimization and helps reduce oscillations near the minimum point.\n",
    "\n",
    "### AdaGrad\n",
    "\n",
    "AdaGrad means __adaptive learning rate__. The algorithm updates the weights as follows\n",
    "\n",
    "$$ \n",
    "   \\begin{align}\n",
    "          s_i' &= s_i + \\left( \\frac{\\partial}{\\partial \\theta_i} J(\\theta) \\right)^2 \\\\\n",
    "     \\theta_i' &= \\theta_i - \\frac{\\eta}{\\sqrt{s_i+\\epsilon}} \\frac{\\partial}{\\partial \\theta_i} J(\\theta)\n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "So the algorithm scales down the learning rate, and does so faster for steeper dimensions than for dimensions with gentler slopes. Hence the name __adaptive learning rate__. Accodring to the Handson book, the technique works well for simple quadratic problems (loss functions?), but often stops too early when training neural networks. In general, AdaGrad should be avoided.\n",
    "\n",
    "### RMSProp\n",
    "\n",
    "RMSProp is proposed by Tieleman and Hinton to fix the problem of AdaGrad. The algorithm goes as follows\n",
    "\n",
    "$$ \n",
    "   \\begin{align}\n",
    "          s_i' &= \\beta s_i + (1-\\beta)\\left( \\frac{\\partial}{\\partial \\theta_i} J(\\theta) \\right)^2 \\\\\n",
    "     \\theta_i' &= \\theta_i - \\frac{\\eta}{\\sqrt{s_i+\\epsilon}} \\frac{\\partial}{\\partial \\theta_i} J(\\theta)\n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "So the $\\mathbf{s}$ vector update is replaced by using exponential decay. $\\beta$ normally is set to be 0.9. It turns out the algorithm is much better than AdaGrad. It also generally performs better than momentum and optimization and Nesterov accelerated gradient.\n",
    "\n",
    "### Adam Optimization\n",
    "\n",
    "Adam stands for __adaptive moment estimation__. The method combines the ideas of momentum optimization and RMSProp:\n",
    "\n",
    "$$ \n",
    "   \\begin{align}\n",
    "     \\mathbf{m}' &= \\beta_1\\,\\mathbf{m} + (1-\\beta_1) \\nabla_\\theta J(\\theta) \\\\   \n",
    "            s_i' &= \\beta_2 s_i + (1-\\beta_2)\\left( \\frac{\\partial}{\\partial \\theta_i} J(\\theta) \\right)^2 \\\\\n",
    "     \\mathbf{m}' &= \\frac{\\mathbf{m}}{1-\\beta_1^T}\\\\\n",
    "     \\mathbf{s}' &= \\frac{\\mathbf{s}}{1-\\beta_2^T}\\\\\n",
    "       \\theta_i' &= \\theta_i - \\frac{\\eta\\,\\mathbf{m}}{\\sqrt{s_i+\\epsilon}}      \n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the method keeps track of\n",
    "1. the exponential decaying average of past gradients.\n",
    "2. the exponential decaying average of past squared gradients.\n",
    "The algorithm is an adaptive learning rate technique, it requires less tuning of the learning rate hyperparameter $\\eta$. The default $\\eta = 0.001$ usually works well.\n",
    "\n",
    "### Learning Rate Scheduling\n",
    "Four methods to tune the learning rate on the fly in order to speed up convergence:\n",
    "1. __Predetermined piecewise constant learning rate__\n",
    "    Set the learning rate every $N$ steps by hand. Works well, but requires fiddling around.\n",
    "\n",
    "2. __Performance scheduling__\n",
    "    Measure the validation error at every $N$ steps and reduce the learning rate by a factor of $\\lambda$ when the error stops dropping.\n",
    "\n",
    "3. __Exponential scheduling__\n",
    "    Schedule $\\eta$ as follows:\n",
    "\n",
    "    $$\\eta(t) = \\eta_0\\,10^{-t/r}$$\n",
    "\n",
    "    where $\\eta_0$ and $r$ are two new hyperparameters.\n",
    "\n",
    "4. __Power scheduling__\n",
    "Use a power-law to schedule $\\eta$:\n",
    "\n",
    "$$ \\eta(t) = \\eta_0(1+t/r)^{-c}$$\n",
    "\n",
    "AdaGrad, RMSProp, and Adam optimization automatically reduce $\\eta$ during training. No training rate scheduling is necessary for these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Avoiding Overfitting Through Regularization<a id=\"Part_4\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Evaluate the model on a validation set at regular intervals, and set a \"winner\" snapshot if it outperforms previous \"winner\" snapshots.\n",
    "\n",
    "### $\\ell_1$ and $\\ell_2$ Regularization\n",
    "\n",
    "Use $\\ell_1$ and $\\ell_2$ rgularizations to constrain a neural network's weights (but typically not its biases).\n",
    "In tensorflow, many functions that create variables, such as `get_variable()` or `fully_connected()` accept a `*_regularizer` argument for each created variable, e.g. `weights_regularizer`. One can pass any function that takes weights as an argument and returns the corresponding regularization loss. An example of the implementation given in the Handson book goes as follows:\n",
    "```python\n",
    "    with arg_scope(\n",
    "            [fully_connected],\n",
    "            weights_regularizer = tf.contrib.layers.l1_regularizer(scale=0.1)):\n",
    "        hidden1 = fully_connected(X, n_hidden1, scope='hidden1')\n",
    "        hidden2 = fully_connected(hidden1, n_hidden2, scope='hidden2')\n",
    "        logits = fully_connected(hidden2, n_outputs, activation_fn = Nono, scope='out')\n",
    "        \n",
    "    reg_loss = tf.get_collection(tg.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_loss, name='loss')\n",
    "```\n",
    "The first six lines of the code create two hidden layers and one output layer. It also creates nodes in the graph to compute $\\ell_1$ regularization loss corresponding to each layer's weights. Tensorflow would add these nodes to a special collection containing all the regularized losses. We need to add these losses to the overall loss. This is done by the last two lines of the code.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "At every training step, every neuron has a probability $p$ of being temporarily dropped out from the network, meaning it will be entirely ignored during this training step, but it might be activated again in the next step. The hyperparameter $p$ is called __dropout rate__. After training, neurons don't get dropped anymore. \n",
    "\n",
    "<img src=\"./images/fig_dropout.png\" width='400'>\n",
    "\n",
    "In tensorflow, use the `dropout` class from `tf.contrib.layers`. This class would correctly turn off the dropout operation during model evaluation.\n",
    "Rules of thumb:\n",
    "- If the model overfits, then increase the dropout rate, i.e. reduce `keep_prob` parameter.\n",
    "- If the model underfits otherwise, decrease the dropout rate.\n",
    "- Reduce the dropout rate for small layers.\n",
    "- Increase the rate for large layers that have lots of neurons.\n",
    "Note that __dropout tend to significantly slow down convergence__, but it usually results in a much better model when tuned properly.\n",
    "\n",
    "### Max-Norm Regularization\n",
    "\n",
    "As the name suggests, the max-norm algorithm cuts the weights $\\mathbf{w}$ of the incoming connections such that $|\\mathbf{w}|_2 \\leq r$, where $r$ is the max-norm hyperparameter and $|\\cdot|_2$ denotes the $\\ell_2$ norm. This is typically implemented in the following way:\n",
    "\n",
    "$$\n",
    "  \\mathbf{w}' = \\mathbf{w}\\cdot\\frac{r}{|\\mathbf{w}|_2}\n",
    "$$\n",
    "\n",
    "Reducing $r$ increases the amount of regularization and helps reduce overfitting.\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "The method consists of generating new training instances from existing ones, artificially boosting the size of the training set. This reduces overfitting, making it a regularization technique.\n",
    "- New instances must be learnable.\n",
    "- One needs to generate realistic training instances that a human cannot tell.\n",
    "- Adding white noise will not help, because noise is not learnable.\n",
    "- It is often preferrable to generate new samples on the fly to save memory.\n",
    "\n",
    "For tasks that involve images, typical ways of generating new instances include: transposing, rotating, resizing, flipping, and cropping, as well as adjusting the brightness, contrast, saturation, and hue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
