{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "### Definition\n",
    "The perceptron is one of the simplest ANN architectures. Its basis a neuron called linear threshold unit (LTU) depicted as follows:\n",
    "\n",
    "<img src=\"./images/fig_LTU.png\" width='300'>\n",
    "\n",
    "The input layer consists of neurons that takes numbers $x_0$, $x_1$, $x_2$, ... as the inputs. The input neurons simply output whatever they are fed. Typically $x_0=1$ serves as the bias term. Each output $x_i$ from the input layer is associated with a weight $w_i$. The LTU computes the weighted sum\n",
    "\n",
    "$$ z = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n = \\mathbf{w}^T\\cdot\\mathbf{x}, $$\n",
    "    \n",
    "then applies the activation function to the sum $z$.\n",
    "\n",
    "Two common activation functions\n",
    "\n",
    "- Heaviside step function \n",
    "\n",
    "$$\n",
    "h(z) = \\left\\{\\begin{array}{cc}\n",
    "               0 & \\mbox{if}\\,\\, z < 0,\\\\\n",
    "               1 & \\mbox{if}\\,\\, z \\geq 0.\n",
    "               \\end{array}\n",
    "        \\right. \n",
    "$$\n",
    "\n",
    "\n",
    "- Sign function\n",
    "\n",
    "    $$ \\mbox{sgn}(z) = \\left\\{\\begin{array}{cc}\n",
    "                 -1 & \\mbox{if}\\,\\, z < 0,\\\\\n",
    "                  0 & \\mbox{if}\\,\\, z = 0, \\\\\n",
    "                  1 & \\mbox{if}\\,\\, z > 0.\n",
    "               \\end{array}\n",
    "        \\right. $$\n",
    "    \n",
    "The perceptron is composed of a single layer of LTUs.\n",
    "\n",
    "<img src=\"./images/fig_perceptron.png\" width='300'>\n",
    "\n",
    "### Perception training\n",
    "\n",
    "How is a perceptron trained? The basic idea is \"__Cells that fire together wire together__,\" which is also refereed to as __Hebb's rule__. The rule states that the connection weight between two neurons is increased whenever they have the same output. The enfore the rule, the weight is updated as follows\n",
    "\n",
    "$$ w_{i,j}' = w_{i,j} + \\eta\\,(\\hat y_j - y_j)\\,x_i, $$\n",
    "\n",
    "where\n",
    "- $w_{i,j}$ is the connection weight between the i-th input neuron and the j-th output neuron.\n",
    "- $x_i$ is the input value of the current training instance.\n",
    "- $\\hat y_i$ is the output of the j-th output neuron for the current training instance.\n",
    "- $j_i$ is the target ouput of the j-th output neuron for the current training instance.\n",
    "- $\\eta$ is the learning rate.\n",
    "\n",
    "So for every output neuron that gives a wrong prediction, the equation reinforces the connection weights from the inputs that would have contributed to the correct prediction.\n",
    "\n",
    "Several notable properties of a single perceptron:\n",
    "- It is a type of feed-forward network.\n",
    "- It is a __binary__ classifier. This is because the output is either positive (+1) or negative (0, in the case of step function or -1, in the case of sign function).\n",
    "- It has a linear decision boundary defined by the condition $\\mathbf{w}^T\\cdot\\mathbf{x}=0$. In other words, the perceptron has only one decision boundary. \n",
    "- Scikit-Learn's perceptron implementation resembles its SGD class. In fact, the SGD classifier with `loss=\"perceptron\"`, `learning_rate=\"constant\"`, `eta0=1`, and `penalty=None` hyperparameters is equivalent to Scikit-Learn's peceptron class.\n",
    "- Peceptrons do not output class probability. They just make predictions based on threshold.\n",
    "\n",
    "The following code is a simple implementation of a single LTU applied to a linearly separable data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['mathtext.fontset'] = 'cm'\n",
    "plt.rcParams['mathtext.rm'] = 'serif'\n",
    "plt.rcParams['figure.figsize'] = (14.0, 8.0)\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial weight: [ -3.79   -2.93   -1.57]\n",
      "\n",
      "Epoch:   1,  error:  0.667,   weight: [ -3.79   -2.38   -1.32]\n",
      "Epoch:   2,  error:  0.833,   weight: [ -3.69   -2.03   -0.97]\n",
      "Epoch:   3,  error:  0.667,   weight: [ -3.49   -1.78   -0.77]\n",
      "Epoch:   4,  error:  0.667,   weight: [ -3.29   -1.53   -0.57]\n",
      "Epoch:   5,  error:  0.667,   weight: [ -3.09   -1.28   -0.37]\n",
      "Epoch:   6,  error:  0.500,   weight: [ -2.79   -1.23   -0.27]\n",
      "Epoch:   7,  error:  0.667,   weight: [ -2.59   -0.98   -0.07]\n",
      "Epoch:   8,  error:  0.500,   weight: [ -2.29   -0.93    0.03]\n",
      "Epoch:   9,  error:  0.500,   weight: [ -1.99   -0.88    0.13]\n",
      "Epoch:  10,  error:  0.500,   weight: [ -1.69   -0.83    0.23]\n",
      "Epoch:  11,  error:  0.500,   weight: [ -1.39   -0.78    0.33]\n",
      "Epoch:  12,  error:  0.333,   weight: [ -1.19   -0.53    0.33]\n",
      "Epoch:  13,  error:  0.500,   weight: [ -0.89   -0.48    0.43]\n",
      "Epoch:  14,  error:  0.333,   weight: [ -0.69   -0.23    0.43]\n",
      "Epoch:  15,  error:  0.500,   weight: [ -0.39   -0.18    0.53]\n",
      "Epoch:  16,  error:  0.333,   weight: [ -0.19    0.07    0.53]\n",
      "Epoch:  17,  error:  0.333,   weight: [  0.01    0.02    0.53]\n",
      "Epoch:  18,  error:  0.167,   weight: [  0.11    0.17    0.43]\n",
      "Epoch:  19,  error:  0.333,   weight: [  0.31    0.12    0.43]\n",
      "Epoch:  20,  error:  0.000,   weight: [  0.31    0.12    0.43]\n",
      "\n",
      "Decision boundary interceptions: \n",
      "x0:  -2.683, y0:  -0.730\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linearly separable Data sets\n",
    "#X = [[1,1], [2,2], [4,4], [5,5], [5,4]]\n",
    "#y = [1, 1, 0, 0, 0]\n",
    "\n",
    "X = [[1,1], [2,-2], [1.5,-1], [-2,1], [-1,-1.5], [-2,-1]]\n",
    "y = [1, 0, 1, 1, 0, 0]\n",
    "\n",
    "# XOR problrm data set\n",
    "#X = [[0,0], [1,0], [0,1], [1,1]]\n",
    "#y = [1, 0, 0, 1]\n",
    "\n",
    "# Add bias term to X\n",
    "Xb = np.c_[np.ones((np.shape(X)[0], 1)), X]    \n",
    "\n",
    "# Initialize random weight\n",
    "weight = 5*np.random.random_sample((3)) - 5\n",
    "eta    = 0.1\n",
    "epoch  = 2000\n",
    "\n",
    "idx = 0\n",
    "print('\\nInitial weight: ['+'  '.join('{:6.2f}'.format(f) for f in weight)+']\\n')\n",
    "# Single perceptron training\n",
    "for iter in range(epoch):\n",
    "    error = 0.\n",
    "    for x, yt in zip(Xb, y):\n",
    "        y_pred = np.heaviside(x.dot(weight), 0.0)\n",
    "        err = yt - y_pred\n",
    "        weight = weight + eta*err*x\n",
    "        error += np.abs(err)/6.\n",
    "    idx += 1\n",
    "    print('Epoch: {0:3d},  error: {1:6.3f},'.format(idx, error) + \\\n",
    "          '   weight: ['+'  '.join('{:6.2f}'.format(f) for f in weight)+']')\n",
    "    \n",
    "    # The data set is linearly separable. So we use zero error as the exit condition.\n",
    "    if (error < 1.e-6):\n",
    "        break\n",
    "\n",
    "print('\\nDecision boundary interceptions: ')\n",
    "print('x0:{0:8.3f}, y0:{1:8.3f}'.format(-weight[0]/weight[1], -weight[0]/weight[2]))\n",
    "\n",
    "# Plot data and decision boundary\n",
    "size = 400\n",
    "xmin = np.min(np.amin(X, axis=0))-1\n",
    "xmax = np.max(np.amax(X, axis=0))+1\n",
    "ymin = np.min(np.amin(X, axis=1))-1\n",
    "ymax = np.max(np.amax(X, axis=1))+1\n",
    "\n",
    "xx, yy = np.meshgrid( np.linspace(xmin, xmax, size), np.linspace(ymin, ymax, size))\n",
    "\n",
    "mesh   = np.c_[xx.ravel(), yy.ravel()]\n",
    "# Add bias term to the mesh\n",
    "meshb  = np.c_[np.ones((mesh.shape[0], 1)), mesh]\n",
    "z  = np.heaviside(meshb.dot(weight), 1.0)\n",
    "zz = np.reshape(z, xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(xx, yy, zz, cmap=plt.cm.brg, alpha=0.1)\n",
    "\n",
    "# Plot data points\n",
    "style = [ 'bo' if x > 0 else 'ro'  for x in y ]\n",
    "for x, c in zip(X, style):\n",
    "    plt.plot(x[0], x[1], c)   \n",
    "plt.hlines(0, xmin, xmax, color='k', lw=0.5)\n",
    "plt.vlines(0, ymax, ymin, color='k', lw=0.5)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR classification problem\n",
    "\n",
    "This the problem of using a neural network to predict the outs of XOR logic gates given two binary inputs. The possible inputs and outs of XOR gates are\n",
    "\n",
    "| input 1 | input 2 | output |\n",
    "|---------|---------|--------|\n",
    "|     0   |     0   |    0   |   \n",
    "|     0   |     1   |    1   |\n",
    "|     1   |     1   |    0   |\n",
    "|     1   |     0   |    1   |\n",
    "\n",
    "This problem can also be demonstrated using the Cartesian coordinate system, shown as follows:\n",
    "\n",
    "<img src=\"./images/fig_XOR.png\" width='240'>\n",
    "\n",
    "The figure clearly shows that there is no way of seperating squares and triangles using one decision boundary. This is true for a single layer perceptron as well as for any linear classification models. __We can also demonstrate this issue by applying the single perceptron model we just built to the XOR data set.__ The training error will never converge to the minimum.\n",
    "\n",
    "A solution to the XOR problem is to stack multiple perceptron layers, forming the so-called multi-layer perceptron (MLP). A MLP shown in the following figure can solve the XOR problem.\n",
    "\n",
    "<img src=\"./images/fig_XOR_MLP.png\" width='240'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron (MLP) and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP is composed of one input layer, one or more layers of LTUs called hidden layers, and one final layer of LTUs called the output layer. The following figure is an example.\n",
    "\n",
    "<img src=\"./images/fig_MLP.png\" width='500'>\n",
    "\n",
    "The most important development in neural network training is the idea of backpropagation algorithm. It could be described as __gradient descent using reverse-mode autodiff.__ Here is a breakdown of the algorithm\n",
    "\n",
    "For each training instance $x$:\n",
    "- Feed $x$ to the network and computes the outputs of every neuron in each layer. That is, forward pass.\n",
    "- Measure the network's output error by comparing the desired output and actual output.\n",
    "- Computes how much each neuron in the last hidden layer contributed to each output neuron's error.\n",
    "- Estimates how much of these error contributions came from the previous layer.\n",
    "- Keep doing the same backward error estimation until the algorithm reaches the input layer.\n",
    "- The last step: apply Gradient Descent on all connection weights using the error gradients measured previously. So the weights get adjusted in order to redue the error.\n",
    "\n",
    "The reverse pass efficiently measures the error gradient across all the connection weights by __propagating the error gradient backward__ in the network. \n",
    "\n",
    "In order for the calculation of gradients to work, one needs to use a differentiable activation function. Common candidates are:\n",
    "- Logistic function $$f(z) = 1/(1+\\exp(-z)).$$\n",
    "- Hyperbolic tangent function $$f(z) = \\tanh(z).$$\n",
    "- ReLU function $$f(z) = \\mbox{max}(0,z).$$\n",
    "\n",
    "An MLP is often used for classification, with each output corresponding to a different binary class. __When the classes are exclusive, the softmax function is often shared across the output layer as the activation function.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, how does backpropagation work? Before we dive into details, let's examine two other accurate numerical approaches for computing gradients.\n",
    "- #### Forward-Mode Autodiff\n",
    "  This method is based on two very simple ideas: dual numbers and Taylor expansion.\n",
    "  \n",
    "  Dual number is a real number of the form $a+b\\epsilon$, where $a$, $b$, and $\\epsilon$ are all real numbers.\n",
    "  In particular, $\\epsilon$ is an infinitesimal number such that it satisfies $\\epsilon^2=0$ within numerical\n",
    "  accuracy.\n",
    "  \n",
    "  Next is the Taylor expansion. For any real differentiable function $f(z)$, we can expan the it around $z=z_0$:\n",
    "\n",
    "  $$ f(z) = f(z_0) + f'(z_0)(z-z_0) + \\frac{1}{2!}f''(z_0)(z-z_0)^2 + \\frac{1}{3!}f'''(z_0)(z-z_0)^3 + \\ldots$$\n",
    "  \n",
    "  So by letting $z=a+\\epsilon$ and $z_0=a$, we have\n",
    "  \n",
    "  $$ f(a+b\\epsilon) = f(a) + f'(a)\\,\\epsilon$$\n",
    "  \n",
    "  Higher order terms vanish because $\\epsilon^2=0$. So by computing $f(a+\\epsilon)$, we automatically get $f'(a)$ \n",
    "  from the coefficient of the $\\epsilon$ term.\n",
    "  \n",
    "  The downside of the forward-mode autodiff method is that for a function with $k$ variables, the number of\n",
    "  calculations required to get $f'(z_i)$ for $\\forall\\, i \\in (1, \\ldots, k)$ is proportional to $k$. So the \n",
    "  computational cost could be expansion for training sets with a huge number of features.\n",
    "  \n",
    "  Now let's look at an example. Suppose we are given a function\n",
    "  \n",
    "  $$f(x, y) = x^2y+y+2,$$\n",
    "  \n",
    "  and we would like to compute its partial derivative $\\partial f/\\partial x$ evaluated a $(x,y)=(3,4)$. Based on the\n",
    "  rule we just discussed, we can proceed as follows\n",
    "  \n",
    "  $$ f(3+\\epsilon, 4) = 4(3+\\epsilon)^2 + 4 + 2 = 42+24\\epsilon,$$\n",
    " \n",
    "  which can be summarized y the following graph. Therefore we immediately have $\\partial f/\\partial x|_{(3,4)} = 24$. \n",
    "  \n",
    "  <img src=\"./images/fig_Forward_Autodiff.png\" width='400'>\n",
    "\n",
    "- #### Reverse-Mode Autodiff\n",
    "  The reverse-mode first works the graph in the forward direction. In the first pass, it computes the \n",
    "  value of every node (labeled by $n_1\\sim n_7$ in the following figure) of the graph. In the second\n",
    "  pass, the method computes all the partial derivatives using __chain rule__. Because the graph maps out all\n",
    "  functional dependency between nodes  \n",
    "  $$\n",
    "  \\begin{align}\n",
    "    n_7 &= n_5 + n_6,\\\\\n",
    "    n_6 &= n_2 + n_3, \\\\\n",
    "    n_5 &= n_4 \\times n_2,\\\\\n",
    "    n_4 &= n_1 \\times n_1.\n",
    "  \\end{align}\n",
    "  $$\n",
    "  \n",
    "  These relations enable us to work out the desired derivaties. For example, $n_7$ is an output \n",
    "  node that simply gives the node's value: $f = n_7$. Therefore \n",
    "  \n",
    "  $$\\frac{\\partial f}{\\partial n_7}=1.$$\n",
    "  \n",
    "  Moving on to the next level, we could compute the derivative with respect to $n_6$ and $n_5$:\n",
    "\n",
    "  $$\n",
    "  \\begin{align}\n",
    "      \\frac{\\partial f}{\\partial n_6} &= \\frac{\\partial f}{\\partial n_7}\\frac{\\partial n_7}{\\partial n_6} \n",
    "                                       = 1 \\times 1 = 1,\\\\\n",
    "      \\frac{\\partial f}{\\partial n_5} &= \\frac{\\partial f}{\\partial n_7}\\frac{\\partial n_7}{\\partial n_5} \n",
    "                                       = 1 \\times 1 = 1.\n",
    "  \\end{align}\n",
    "  $$\n",
    "  \n",
    "  Similarly, we have:\n",
    "\n",
    "  $$\n",
    "  \\begin{align}\n",
    "      \\frac{\\partial f}{\\partial n_4} &= \\frac{\\partial f}{\\partial n_5}\\frac{\\partial n_5}{\\partial n_4} \n",
    "                                       = 1 \\times n_2 = 4, \\\\\n",
    "      \\frac{\\partial f}{\\partial n_3} &= \\frac{\\partial f}{\\partial n_6}\\frac{\\partial n_6}{\\partial n_3} \n",
    "                                       = 1 \\times 0 = 0, \\\\                                   \n",
    "      \\frac{\\partial f}{\\partial n_2} &= \\frac{\\partial f}{\\partial n_6}\\frac{\\partial n_6}{\\partial n_2} +\n",
    "                                         \\frac{\\partial f}{\\partial n_5}\\frac{\\partial n_5}{\\partial n_2}\n",
    "                                       = 1 \\times 1 + 1\\times n_4 = 10, \\\\\n",
    "      \\frac{\\partial f}{\\partial n_1} &= \\frac{\\partial f}{\\partial n_4}\\frac{\\partial n_4}{\\partial n_1} \n",
    "                                       = 4 \\times 2n_1 = 24.\n",
    "  \\end{align}\n",
    "  $$\n",
    "\n",
    "  We can see that by the time we reach the input nodes $n_1=x$ and $n_2=y$, all the necessary partial\n",
    "  derivatives have been computed. As a result, we can immediately obtain \n",
    "  $\\partial f/\\partial x|_{(3,4)} = 24$ and $\\partial f/\\partial y|_{(3,4)} = 10$.\n",
    "  \n",
    "  The reverse-mode autodiff method for the graph $f(x,y)=x^2y+y+2$ is summarized by the following graph\n",
    "\n",
    "  <img src=\"./images/fig_Reverse_Autodiff.png\" width='600'>\n",
    "  \n",
    "  The reverse-mode autodiff is powerful in that it only needs one pass for an arbitrary number of inputs.\n",
    "  As a result, the ${\\cal O}(n_{outputs}+1)$ computational cost is much less than the forward-mode autodiff\n",
    "  ${\\cal O}(n_{inputs})$ for a dataset that have a huge number of input features (here $n_{inputs}$ and \n",
    "  $n_{outputs}$ are the number of inputs and outputs of the graph).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Backpropagation, in greater detail\n",
    "\n",
    "Now it is time to describe the idea of backpropagation. We will use the following neural network structure as \n",
    "an example.\n",
    "\n",
    "<img src=\"./images/fig_backpropagation.png\" width='550'>\n",
    "\n",
    "The network consists of an input layer with $K$ neurons $x_1,\\ldots, x_K$, an output layer with $M$ neurons \n",
    "$y_1,\\ldots, y_M$. The hidden layer has $N$ neurons $h_1,\\ldots, h_N$. The weights between input-hidden and\n",
    "hidden-output layers are characterized collectively by $\\{w_{ki}\\}$ and $\\{w'_{ij}\\}$. The activation function\n",
    "$\\varphi(z)$ is smooth and differentiable. As described above, there are several options. Here we use the logistic\n",
    "function\n",
    "\n",
    "$$ \\varphi(z) = \\frac{1}{1+e^{-z}}, $$\n",
    "\n",
    "whose first derivative has a nice form\n",
    "\n",
    "$$ \\varphi'(z) = \\varphi(z)\\left( 1-\\varphi(z) \\right). $$\n",
    "\n",
    "An output neuron, say, $y_j$, takes $h_1$, $h_2$, $\\ldots$, $h_N$ from the hidden layer and form\n",
    "a weighted sum\n",
    "\n",
    "$$u'_j = \\displaystyle\\sum_{i=1}^N w'_{ij}\\,h_i.$$\n",
    "\n",
    "$u'_j$ is then fed to the activation function in order to decide the output $y_j$:\n",
    "\n",
    "$$ y_j = \\varphi(u'_j).$$\n",
    "\n",
    "The same procedure applies to other layer outputs.\n",
    "\n",
    "The purpose of network training is to find a set of weights $\\{w_{ki}\\}$ and $\\{w'_{ij}\\}$ so that for a given\n",
    "set of training instances, the output error $E$ can be as low as possible. A typical mearure of $E$ is the mean\n",
    "squared error\n",
    "\n",
    "$$ E = \\frac{1}{2M}\\sum_{j=1}^M\\,\\left( y_j - t_j \\right)^2, $$\n",
    "\n",
    "where $t_j$ is the target output of the $j$-th output neuron. If one plots $E$ as a function of $y_j$ and $t_j$,\n",
    "the result will be a paraboloid, which has a global minimum. In this sense, deep learning (or in general, machine\n",
    "learning) at the very heart is an optimization (minimization) problem. The most common method for searching\n",
    "the minimum is gradient descent. For this purpose, we need to compute gradients of $E$ with respect to all the\n",
    "weights. This is when the idea of backpropagation comes into play.\n",
    "\n",
    "Let's begin with the output layer and compute the drivative of $E$ with respect to $w'_{ij}$. Recall that $E$ is\n",
    "a function of $y_j$ which in turn depends on $w'_{ij}$ and $h_i$ through the activation function. Therefore\n",
    "\n",
    "$$ \n",
    "   \\frac{\\partial E}{\\partial w'_{ij}} = \\frac{\\partial E}{\\partial y_j}\\,\n",
    "                                         \\frac{\\partial y_j}{\\partial u'_j}\\,\n",
    "                                         \\frac{\\partial u'_j}{\\partial w'_{ij}}. \n",
    "$$\n",
    "\n",
    "Recall that $y_j = \\varphi(u'_j)$ and that $u'_j$ is just a weighted sum of $h_i$, we have\n",
    "\n",
    "$$\n",
    "   \\frac{\\partial E}{\\partial w'_{ij}} = \\frac{\\partial E}{\\partial y_j}\\cdot\n",
    "                                        y_j\\cdot(1-y_j)\\cdot h_i.\n",
    "$$\n",
    "\n",
    "With this gradient, the weight $w'_{ij}$ is updated as\n",
    "\n",
    "$$ w'^{,new}_{ij} = w'^{,old}_{ij} - \\eta \\cdot \\frac{\\partial E}{\\partial y_j}\\cdot\n",
    "                                        y_j\\cdot(1-y_j)\\cdot h_i. $$\n",
    "\n",
    "Now to compute the derivative with respect to the weights that couple the input and hidden layers, we proceed\n",
    "as follow.\n",
    "\n",
    "$$\n",
    "  \\frac{\\partial E}{\\partial w_{ki}} = \\sum_{j=1}^M\\,\n",
    "                                         \\left( \\frac{\\partial E}{\\partial y_j}\\,\n",
    "                                         \\frac{\\partial y_j}{\\partial u'_j}\\,\n",
    "                                         \\frac{\\partial u'_j}{\\partial h_i}\\right)\\,\n",
    "                                         \\frac{\\partial h_i}{\\partial u_i}\\,\n",
    "                                         \\frac{\\partial u_i}{\\partial w'_{ij}}.\n",
    "$$\n",
    "\n",
    "The summation is required in order to take into account the fact that the hidden unit that $w_{ki}$ connects to is\n",
    "itself connected to every output unit. Using the definition of $u'_j$ and $u_i$, we arrive at the following equation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ki}} = \\sum_{j=1}^M\\,\n",
    "                                         \\left(\\frac{\\partial E}{\\partial y_j}\\cdot\n",
    "                                         y_j \\cdot (1-y_j)\\cdot w'_{ij} \\right)\\cdot\n",
    "                                         h_i\\cdot (1-h_i) \\cdot x_k.\n",
    "$$\n",
    "\n",
    "With the gradient vector in hand, we can update the weight $w_{ki}$ accordingly\n",
    "\n",
    "$$ w^{new}_{ki} = w_{ki}^{old} - \\eta\\cdot \\sum_{j=1}^M\\,\n",
    "                                         \\left(\\frac{\\partial E}{\\partial y_j}\\cdot\n",
    "                                         y_j \\cdot (1-y_j)\\cdot w'_{ij} \\right)\\cdot\n",
    "                                         h_i\\cdot (1-h_i) \\cdot x_k.\n",
    "$$\n",
    "\n",
    "The same procedure is repeated if there are more than one hidden layers until the input layer is reached.\n",
    "Because the error gets propagated in reversed direction, from output all the way to input neurons, the \n",
    "algorithm is hence called backpropagation. This, in essense, is the reverse-mode autodiff we just saw\n",
    "in the earlier discussion. In that example, the computation of $\\partial f/\\partial n_2$ needs to take\n",
    "into account the contribution from both nodes $n_5$ and $n_6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
