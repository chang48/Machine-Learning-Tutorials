{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with text and sequences: Recurrent Neural Networks<a id=\"Top\"></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "Table of Content\n",
    "<ul>\n",
    "<li>1. Recurrent Neural Networks</li>\n",
    "<li>2. <a href=\"#Part_2\">RNN layers</a></li>     \n",
    "    <ul>\n",
    "        <li>2.1 <a href=\"#Part_2_1\">SimpleRNN layers</a></li>  \n",
    "        <li>2.2 <a href=\"#Part_2_2\">LSTM layers</a></li>\n",
    "        <li>2.3 <a href=\"#Part_2_3\">GRU layers</a></li>\n",
    "        <li>2.4 <a href=\"#Part_2_4\">Notes on the usage of Keras' RNN layers</a></li>\n",
    "    </ul>    \n",
    "<li>3. <a href=\"#Part_3\">Working with text data: Embedding</a></li>    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "So far we have covered two classes of neural net architectures: fully connected networks and convolutional \n",
    "neural networks. They are often referred to as _feedforward networks_. Fully connected and convolutional \n",
    "neural networks have no memory, in a sense that \n",
    "\n",
    "1. Each input shown to the networks is processed independently.\n",
    "2. No states are kept in between inputs.\n",
    "\n",
    "So, to process a text sequence or temporal series using such networks, it is necessary to turn the entire \n",
    "sequence or series into a single data point. For example, convert an IMDB or Amazon review into a sequence \n",
    "of encoded numbers; or use the entire time series as an input. While this practice might work when the \n",
    "length of data is fixed, it quickly become inconvenient if the data have a variable size/length.\n",
    "\n",
    "However, there is a more fundamental reason why a feedforward network is not the best candidate for \n",
    "processing sequential/serial data. Think about this: more often than not, neighboring data points in a \n",
    "text dataset or a temporal series often have some logical connection. For example, the sentence you \n",
    "are reading consists of words that have a certain semantic order. Moreover, the sentence is processed \n",
    "word by word while keeping memories of what came before. While you read, there exists some sort of \n",
    "memory states between data points.\n",
    "\n",
    "A recurrent neural network (RNN) is designed to exploit the internal connections/states in between \n",
    "data points in a sequence. Using Francois Chollet's words: __a RNN processes sequences by iterating \n",
    "through the sequence elements and maintaining a state containing information relative to what \n",
    "it has seen so far__. Effectively, each neuron in an RNN has an internal loop that sends the output \n",
    "in the previous step back to itself as an additional input for the next step, as indicated by panel (a) \n",
    "of the diagram below:\n",
    "\n",
    "<img src='./images/fig_RNN-01.png' width=650>\n",
    "\n",
    "More specifically, at time step $t$, the neuron receives inputs \n",
    "$\\mathbf{x}_{(t)} = (x_{1, (t)}, \\ldots, x_{n_i,(t)})$ and its output from the previous time step \n",
    "$y_{(t-1)}$. Here $\\mathbf{x}_{(t)}$ is a vector with dimension $n_i$, the number of input features. \n",
    "The neuron then forms a weighted sum of the inputs and produce a new output $y_{(t)}$. If we represent \n",
    "the operation along the $t$ axis, we obtain panel (b) of the diagram above. This is referred to as \n",
    "__unrolling the network through time__. One can apply the same idea to a layer of recurrent neurons:\n",
    "\n",
    "<img src='./images/fig_RNN-02.png' width=700>\n",
    "\n",
    "where the vector $\\mathbf{y}_{(t)} = (y_{1,(t)}, y_{2,(t)}, \\ldots)$ represents neuron outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RNN layers<a id=\"Part_2\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>\n",
    "\n",
    "Let's recap by saying that __an RNN is a loop that reuses quantities computed in the previous ineration__. \n",
    "\"Loop\" means the procedure of unrolling over time. Here \"the quantity computed in the previous iteration\" \n",
    "is often referred to as states or hidden states, denoted as $\\mathbf{h}$. So we can summarize an RNN's \n",
    "general structure in the following diagram\n",
    "\n",
    "<img src='./images/fig_RNN-03.png' width=500>\n",
    "\n",
    "Here the yellow square box represents an RNN layer. Some might call it a cell. Depending on how the state variable\n",
    "$\\mathbf{h}$ is handled, there are three primary RNN layer architectures: `SimpleRNN`, `LSTM`, and `GRU` layers. \n",
    "Let's walk through them one by one.\n",
    "\n",
    "## 2.1 SimpleRNN layers<a id=\"Part_2_1\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>\n",
    "\n",
    "As the name implies, the `SimpleRNN` layer is the simplest RNN implementation among the three ones. \n",
    "The state in the `SimpleRNN` cell at time $t$ is just the output of the cell: \n",
    "$\\mathbf{h}_{(t)} = \\mathbf{y}_{(t)}$. \n",
    "For a single input instance $\\mathbf{x}_{(t)}$, the `SimpleRNN` cell first forms a weighted sum of \n",
    "$\\mathbf{h}_{(t)}$ and $\\mathbf{x}_{(t)}$, then send the results to an activation function:\n",
    "\n",
    "$$ \\mathbf{y}_{(t)} = \\phi\\left( \\mathbf{x}_{(t)}^T\\cdot\\mathbf{w}_x + \n",
    "                                 \\mathbf{h}_{(t-1)}^T\\cdot\\mathbf{w}_h + \n",
    "                                 b\n",
    "                           \\right), $$\n",
    "\n",
    "where $\\phi(\\ldots)$ is the hyperbolic tangent activation function. \n",
    "Schemically, the operation can be summarized by the following diagram:\n",
    "\n",
    "<img src='./images/fig_RNN-SimpleRNN.png' width=450>\n",
    "\n",
    "For a batch of input features $\\mathbf{X}_{(t)}$ of shape `(batch_size, input_features)`, the equation \n",
    "generalizes to\n",
    "\n",
    "$$ \\mathbf{Y}_{(t)} = \\phi\\left( \\mathbf{X}_{(t)}\\cdot\\mathbf{W}_x + \n",
    "                                 \\mathbf{h}_{(t-1)}\\cdot\\mathbf{W}_h + \n",
    "                                 \\mathbf{b} \n",
    "                          \\right),$$\n",
    "\n",
    "with $\\mathbf{h}_{(t-1)} = \\mathbf{Y}_{(t-1)}$. Each tensor that appears in this equation has the following \n",
    "shape:\n",
    "- $\\mathbf{Y}_{(t)} = \\mathbf{h}_{(t)}$ : $m\\times n_n,$\n",
    "- $\\mathbf{X}_{(t)}$ : $m\\times n_i,$\n",
    "- $\\mathbf{W}_x$ : $n_i \\times n_n,$\n",
    "- $\\mathbf{W}_h$ : $n_n\\times n_n,$\n",
    "- $\\mathbf{b}$ : $n_n.$  \n",
    "\n",
    "Here $m$ is the size of the input batch. $n_n$ is the number of neurons in the layer, $n_i$ is the\n",
    "number of input features. \n",
    "\n",
    "To make the concept more clearer, we can implement the `SimpleRNN` operation using NumPy. The code\n",
    "will take a sequence of inputs. At each time step, there will only be one input instance. As a result,\n",
    "the input tensor has shape `(timesteps, input_features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 100 \n",
    "input_features = 32 \n",
    "n_neurons = 64\n",
    "\n",
    "inputs = np.random.random((timesteps, input_features))\n",
    "\n",
    "state_t = np.zeros((n_neurons,))\n",
    "\n",
    "Wx = np.random.random((n_neurons, input_features)) \n",
    "Wy = np.random.random((n_neurons, n_neurons)) \n",
    "b  = np.random.random((n_neurons,))\n",
    "\n",
    "successive_outputs = [] \n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wy, state_t) + b)\n",
    "    successive_outputs.append(output_t)\n",
    "    state_t = output_t\n",
    "\n",
    "final_output_sequence = np.concatenate(successive_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(successive_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(final_output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, _unrolling in time_ is realized by the for loop: in each iteration, `input_t` picks up \n",
    "an instance at time $t$ from the tensor `inputs`, the result of hyperbolic tangent activation function \n",
    "is sent to `output_t`, which is then assigned to the cell's state at $t$ `state_t`. Note that\n",
    "because the states don't exist at $t=0$, `state_t` is initialized as a zero tensor at the beginning. \n",
    "Finally, the network's output has shape (100, 64) which stores the output from 64 neurons at each time \n",
    "step. \n",
    "\n",
    "### Keras `SimpleRNN` layer\n",
    "\n",
    "The program we just implemented actually corresponds to an actual Keras layer: the `SimpleRNN` layer:\n",
    "```python\n",
    "    from keras.layers import SimpleRNN\n",
    "```\n",
    "Instead of taking one input feature instance, the `SimpleRNN` layer now takes a batch of inputs of shape\n",
    "`(batch_size, timesteps, input_features)`. The layer's output has two modes:\n",
    "1. The full sequences of successive outputs for each timestep, just like the NumPy example.\n",
    "2. Only the last output for each input sequence.\n",
    "\n",
    "These two modes are controlled by the `return_sequences` constructor argument. Later in the notebook, we'll\n",
    "learn the reason of having these two modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 LSTM layers<a id=\"Part_2_2\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>\n",
    "\n",
    "While `SimpleRNN` is easy to understand, in reality it is too simplistic to achineve any good performance.\n",
    "`SimpleRNN` has a major problem: even though in theory it is supposed to retain at any timestep about inputs \n",
    "seen several timesteps before, `SimpleRNN` in fact cannot learn such information. The reason is due to the\n",
    "__vanishing gradient problem__ during training.\n",
    "\n",
    "The Long Short-Term Memory (LSTM) layer is proposed by Hochreiter and Schmidhuber to address the\n",
    "vanishing gradient problem. Its basic structure is like the `SimpleRNN` layer. On top of that, the LSTM\n",
    "layer adds a way to carry information acress many timesteps. This special information channel prevents\n",
    "signals from gradually vanishing during processing.\n",
    "\n",
    "The architecture of the LSTM layer is depicted in the diagram below\n",
    "\n",
    "<img src='./images/fig_RNN-LSTM.png' width=700>\n",
    "\n",
    "Hochreiter and Schmidhuber's insight is the addition of the extra dataflow $\\mathbf{c}_{(t)}$, as indicated\n",
    "by the red flow lines in the diagram. It will also affect the state sent to the next timestep. The computation \n",
    "of $\\mathbf{c}_{(t)}$ involves three distinct transformations. Each of the transformation has its own weight \n",
    "and bias matrices:\n",
    "\n",
    "$$ \\begin{align}\n",
    "       \\mathbf{i}_{(t)} &= \\sigma\\left( \\mathbf{x}_{(t)}^T\\cdot\\mathbf{w}_{xi} +\n",
    "                                        \\mathbf{h}_{(t-1)}^T\\cdot\\mathbf{w}_{hi} + \n",
    "                                        b_i \\right), \\\\\n",
    "       \\mathbf{f}_{(t)} &= \\sigma\\left( \\mathbf{x}_{(t)}^T\\cdot\\mathbf{w}_{xf} +\n",
    "                                        \\mathbf{h}_{(t-1)}^T\\cdot\\mathbf{w}_{hf} + \n",
    "                                        b_f \\right), \\\\\n",
    "       \\mathbf{g}_{(t)} &= \\tanh\\left(  \\mathbf{x}_{(t)}^T\\cdot\\mathbf{w}_{xg} +\n",
    "                                        \\mathbf{h}_{(t-1)}^T\\cdot\\mathbf{w}_{hg} + \n",
    "                                        b_g \\right),\n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "where $\\sigma(\\ldots)$ is the sigmoid activation function. Then $\\mathbf{c}_{(t)}$ is obtained by\n",
    "\n",
    "$$ \\mathbf{c}_{(t)} = \\mathbf{f}_{(t)}\\otimes\\mathbf{c}_{(t-1)} + \\mathbf{i}_{(t)}\\otimes\\mathbf{g}_{(t)}.$$\n",
    "\n",
    "The symbol $\\otimes$ denotes the element-wise multiplication. So basically, the flow of \n",
    "$\\mathbf{c}_{(t)}$ is regulated by $\\mathbf{f}_{(t)}$ and $\\mathbf{i}_{(t)}$. Both rely on \n",
    "the input connection as well as the recurrent connection. More specifically\n",
    "- $\\mathbf{f}_{(t)}$ controls which part of $\\mathbf{h}_{(t-1)}$ is retained.\n",
    "- $\\mathbf{i}_{(t)}$ determines how much of $\\mathbf{g}_{(t)}$ is added to $\\mathbf{c}_{(t)}$.\n",
    "\n",
    "Finally, the output $\\mathbf{y}_{(t)}$ is obtained by mixing the information from the flow $\\mathbf{c}_{(t)}$\n",
    "and the activation from the inputs $\\mathbf{x}_{(t)}$ and previous state $\\mathbf{h}_{(t-1)}$:\n",
    "\n",
    "$$ \\mathbf{y}_{(t)} = \\mathbf{h}_{(t)} = \n",
    "                      \\sigma\\left( \\mathbf{x}_{(t)}^T\\cdot\\mathbf{w}_x + \n",
    "                                   \\mathbf{h}_{(t-1)}^T\\cdot\\mathbf{w}_h + b \\right) \\otimes\n",
    "                      \\tanh\\left( \\mathbf{c}_{(t)} \\right)\n",
    "                                   $$\n",
    "                                   \n",
    "This makes sure that the output $\\mathbf{y}_{(t)}$ as well as state $\\mathbf{h}_{(t)}$ will \n",
    "\"remember\" the past information encoded in $\\mathbf{c}_{(t)}$.\n",
    "\n",
    "In Keras, the LSTM layers can be implemented by importing\n",
    "```python\n",
    "    from keras.layers import LSTM\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 GRU layers<a id=\"Part_2_3\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>\n",
    "\n",
    "The Gated Recurrent Unit (GRU) layer was proposed by Cho et al. in a\n",
    "<a href='https://arxiv.org/pdf/1406.1078.pdf'>2014 paper</a>. The GRU layer is a simplified version of the\n",
    "LSTM layer. Its architecture is shown in the following diagram\n",
    "\n",
    "<img src='./images/fig_RNN-GRU.png' width=600>\n",
    "\n",
    "The diagram is different from Figure 14-14 in the __Hands-on Machine Learning__ book by Géron. In Figure 14-14,\n",
    "the \"$1-$\" operation is applied to $\\mathbf{z}_{(t)}$ before it is sent to merge with $\\mathbf{h}_{(t-1)}$.\n",
    "However in the original paper, the \"$1-$\" operation is applied to $\\mathbf{z}_{(t)}$ that is sent to merge\n",
    "with $\\mathbf{g}_{(t)}$. Although Géron argued in the \n",
    "<a href='https://www.oreilly.com/catalog/errata.csp?isbn=0636920052289'>Errata</a> \n",
    "that both implementations work. Anyways, the _reset_ $\\mathbf{r}_{(t)}$ and _update_ $\\mathbf{z}_{(t)}$\n",
    "regulators are given by the following equations\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\mathbf{z}_{(t)} &= \\sigma\\left( \\mathbf{x}_{(t)}^T\\cdot\\mathbf{w}_{xz} +\n",
    "                                     \\mathbf{h}_{(t-1)}^T\\cdot\\mathbf{w}_{hz} \\right), \\\\\n",
    "    \\mathbf{r}_{(t)} &= \\sigma\\left( \\mathbf{x}_{(t)}^T\\cdot\\mathbf{w}_{xr} +\n",
    "                                     \\mathbf{h}_{(t-1)}^T\\cdot\\mathbf{w}_{hr} \\right). \\\\       \n",
    "\\end{align}$$\n",
    "\n",
    "The actual output $\\mathbf{h}_{(t)}$ is then computed by\n",
    "\n",
    "$$ \\mathbf{h}_{(t)} =  \\left( 1 - \\mathbf{z}_{(t)} \\right)\\otimes\\mathbf{h}_{(t-1)} +\n",
    "                       \\mathbf{z}_{(t)}\\otimes\\mathbf{g}_{(t)}, $$\n",
    "                       \n",
    "where\n",
    "\n",
    "$$  \\mathbf{g}_{(t)} = \\tanh \\left( \\mathbf{x}_{(t)}^T\\cdot\\mathbf{w}_{xg} +\n",
    "                                     \\left(\\mathbf{r}_{(t)}\\otimes\\mathbf{h}_{(t-1)}\\right)^T\\cdot\\mathbf{w}_{hg}\n",
    "                              \\right).  $$\n",
    "                              \n",
    "Note that in these activations, there is no bias term. The idea behind the GRU design is\n",
    "- The update $\\mathbf{z}_{(t)}$ controls  controls how much information from the previous hidden \n",
    "  state $\\mathbf{h}_{(t-1)}$ will carry over to the current hidden state.\n",
    "- The reset $\\mathbf{r}_{(t)}$ effectively allows the hidden state to drop any information that is\n",
    "  found to be irrelevant in the fiture, allowing more compact representation.\n",
    "  \n",
    "It is argued in the paper that since the hidden state has separate reset and update operations, each hidden\n",
    "unit will learn to capture dependencies over different time scales (short-term and long-term scales). Therefore,\n",
    "the GRU has a simpler design than the LSTM unit while maintaining the ability to \"remember\" the history at\n",
    "different time scales.\n",
    "\n",
    "In Keras, the GRU layer can be implemented by importing:\n",
    "```python\n",
    "    from keras.layers import GRU\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Notes on the usage of Keras' RNN layers<a id=\"Part_2_4\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>\n",
    "\n",
    "As mentioned previously, Keras has three RNN layers:\n",
    "- `keras.layers.SimpleRNN()`\n",
    "- `keras.layers.LSTM()`\n",
    "- `keras.layers.GRU()`\n",
    "\n",
    "These layers in general take inputs of shape `(batch_size, timesteps, input_features)`. However, if you look at\n",
    "the <a href='https://keras.io/layers/recurrent/'>Keras Documention</a> of recurrent layers, you'll find that\n",
    "there is no such thing as input shape in the three layers. What happened? It turns out that `SimpleRNN`,\n",
    "`LSTM`, and `GRU` laysers have all inhereted the base recurrent layer class `RNN` which takes 3D tensors with\n",
    "shape `(batch_size, timesteps, input_dim)` as its input (here `input_dim` = `input_features`). __However, so long \n",
    "as the RNN layer is not used as the first layer in a network, Keras will infer the input shape from the previous\n",
    "layer. In this case, one only needs to specify the dimension of RNN layer output, which is the `units` argument__. \n",
    "If any of the RNN layers is used as the first layer, then one has to specify at least `input_dim` in the layer\n",
    "definition. This sets one of the dimension of the input tensor. Below is a code example from \n",
    "<a href='https://keras.io/getting-started/sequential-model-guide/'>Keras Sequential Model Guide</a>:\n",
    "```python\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense\n",
    "    import numpy as np\n",
    "\n",
    "    data_dim = 16\n",
    "    timesteps = 8\n",
    "    num_classes = 10\n",
    "\n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(32, return_sequences=True))      # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(32))                             # return a single vector of dimension 32\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])```\n",
    "              \n",
    "So in this example, we are stacking three LSTM layers. In the first LSTM layer, one has to write out\n",
    "the shape of input explicitly. Here the `batch_size` is assumed, just like it is in the `Conv2D` layer\n",
    "we saw before. This network has the following architecture:\n",
    "\n",
    "<img src='./images/fig_RNN-Stacked-LSTM.png' width=400>\n",
    "\n",
    "When working with text data, it is necessary to tokenize text input. This is a procedure of transforming\n",
    "text into numeric tensors. In this situation, it is customary to build a network using Keras `Embedding`\n",
    "layer as the first layer of the model. The `Embedding` layer takes input tensor of shape \n",
    "`(batch_size, sequence_length)` and returns a 3D tensor of shape `(batch_size, sequence_length, output_dim)`,\n",
    "which is the input shape of Keras RNN layers. Here is an example using `Embedding` and `LSTM` layers:\n",
    "```python\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Embedding, LSTM\n",
    "\n",
    "    model = Sequential() \n",
    "    model.add(Embedding(max_features, 32)) \n",
    "    model.add(LSTM(32)) \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['acc'])```\n",
    "\n",
    "Now the `LSTM` layer definition, one only needs to define output unit size. The input shape will be \n",
    "inferred from the output of the `Embedding` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Working with text data: Embedding<a id=\"Part_2\"></a>\n",
    "<a href=\"#Top\">Back to page top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
